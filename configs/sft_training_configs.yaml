dataset_text_field: "text"
per_device_train_batch_size: 1
gradient_accumulation_steps: 1  # Use GA to mimic batch size!
warmup_steps: 5
num_train_epochs: 2             # Set this for 1 full training run.
learning_rate: 2e-4             # Reduce to 2e-5 for long training runs
logging_steps: 5
optim: "adamw_8bit"
weight_decay: 0.001
lr_scheduler_type: "linear"
seed: 3407
report_to: "none"               # Use TrackIO/WandB etc